# -*- coding: utf-8 -*-
"""Pandas .ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1S11LKPa4MRtj-PtCjocYL4jh3e9ozW1B

- Python Event (Pandas) by Ibrahim

- 19 March 2022

**Introduction**
- *pandas* as a library in python
- analyze sales data in the past company
- for bigger data analysis

$\text{conda install}$

**Create the virtual environment**
- conda version installed ``conda -V``
- (base) is the default environment
- create a virtual environment ``conda create -n virtualenvironmentname python=3.8``
- click ``y`` to run some simple installation
- anaconda allows python version running without fighthing
- it allows package working 
- we are using python 3.8

**Activate the environment**
- ``conda activate virtualenvironmentname``

**Finding the information of virtual environment**
- ``conda infor --envs``
- ``cls``

**Jupyter Lab as an interactive IDE**
- installing in virtual environment
- ``conda install -c conda-forge jupyterlab``
- ``y``

**Installing pandas with python**
- ``pip install pandas``
- ``pip list``

**Starting Jupyter lab**
- ``cd path``
- ``ls``
- ``jupyter-lab`` (inside your virtual environment)
- ``Ctrl + C`` to close
- jupyter is running on the terminal
- left side is the file directory
- notebook with cells to run code, the output will be seen imidiately, cells can be run separately
- ipynb : interactive python notebook

**Command in cells**
- just write the variables name
- shift enter to go to next cell
- control enter to be cell locally to run

**When and Why**
- dataset can fit into your ram
- excel is too small to run the files
- import data into workspace
- ``import pandas as pd``
- ``import numpy as np``
- ``import matplotlib.pyplot as plt``

**Conventional Database**
1. excel tables with rows and columns
2. columns are features
3. rows are groups
4. dataframe is the tables
5. basic building block of dataframe is a series, is an array of data (whole columns
  - ``pd.Series([23, 41, 6, 5])``
  - it is a special data type with not same as ordinary list, it comes with row / column index now, starting with 0, storing the datatypes
  - datatype is efficient for certain things, default functionality in pandas
  - ``pd.Series([23, 41.0, 6, 5])``: everything is float now
  - ``pd.Series([23, "41.0", 6, 5])``: everything is object (a collection of different data class/objects) now


**Convert data type**
* ``pd.to_numeric(pd.Series(["41.2", 12]))``
* ``pd.to_datetime(pd.Series)``

**Pandas Data Frame** as a collection / combination of *data series* as the building block of data frame
- ``pd.DataFrame(data)``: create table with those data
  - pass in a dictionary
  - ``data = {
    "person":["Iz", "KH", "Avocado", "DK"],
    "fav_icecream":["straw", "water", avacado", "dambal"]
  }``
- ``pd.DataFrame()`` to create an empty data frame

**Reading Data**
- ``%%time``: for performance analysis
- ``pd.read_csv("filename.csv")
- click ``tab`` for auto complete
- datatype usually comes in columns-oriented storage (all A first) or row-oritented storate (A1, B1, A2, B2)
"""

import pandas as pd

height = pd.read_csv("height_country.csv")
internet = pd.read_csv("/content/internet broadband and mobile speeds by country.csv")
tiktok = pd.read_csv("tiktok_top_1000.csv")

# Commented out IPython magic to ensure Python compatibility.
# %time
# big_data = pd.read_parquet("2m-sales-Recors")
# error - running ``!pip install fastparquet`` : this is a console command not a python command

"""**Saving Into Excel Files**
- ``!pip install openpyxl``
- ``dataframe.to_excel("datafilename.xlsx")``
- for compression purpose, then you can read your excel file already
- can read data from web things
- https://eforexcel.com/wp/downloads-18-sample-csv-files-data-sets-for-testing-sales/

**Web Data**
- ``dictionary = pd.read_csv("https://raw.githubusercontent.com/dwyl/english-words/master/words.txt",names=['word'],sep=" ")``
- online files

**Preview and Viewing Data**
- getting information about your dataset
- getting the first five columns: ``dataframe.head()``
- ``dataframe.tail(n)``
- ``dataframe.info()``: overview of data
- object is as string

**Session Two**

**Taking Subsession of Data**
- ``dataframe["column name"]``

**Selecting Multiple column**

**Method 1**

- ``dataframe[["column1", "column2", "column"]]]``
- multiple column default turn into a table

**Method 2 : loc attribute**
- ``dataframe.loc[:, "column1":"column3"]``
- passing in the row and column 
- as a fileter to access subdata
- ``:`` means everything inside the rows or list or columns

**Method 3: "iloc" index loc attribute**
* ``dataframe.iloc[:, [1,2,3]]``
- does not include the last index that you pass in

**Droping columns**
- ``dataframe.drop(columns=["Order ID"], axis=1).info()``
- default is to preserve the dataframe that you have, it copy and do the functions, and return the output after the functions are executed
- to edit the dataframe directly
by ``inplace=True`` passing this argument inside, but this argument does not return output


**Filtering based on criteria**
"""

# height.head()
a = [1,2,3,4,5]
print(a[1])

# Select only the country name
type(height["Country Name"])
height["Rank"] # column name is case sensitive

"""**Exercise 1**
: Make a DataFrame with only Country, Broadband Speed Rank and Broadband Speed using:
1. drop
2. select multiple columns
3. loc
"""

internet.loc[:, "Country":"Broadband Mbps"]

# it has to be list inside a list
print(internet.keys()) # to return all the column names
internet[["Country", "Broadband Speed Rank", "Broadband Mbps"]]

internet.drop(columns=["Mobile Speed Rank", "Mobile Mbps", "As of"])

"""**Imported Data**

**Data Structure Understanding**

**Filtering**

**Cleaning Data**
1. Remove duplicate
2. clear NA
  - by assigning value
  - by remove the entire row
  - converting to zero to ensure not interfere your calculations
"""

missing = pd.read_csv("missing.csv")

missing.info()

# or using dot if there is no space in your column name
print(missing["Column3"].isnull())
missing[missing["Column3"].isnull()] 
# return true if it is null, nan means not a number or invalue
# pass in the condition to be met, and return the condition that satisfy the condition (true) statement that to be returned

# pandas is using boolean operator
missing[
        (missing["Column3"].isnull())
        | (missing["Column4"].isnull())
        ]

# filling the value
# fill anywhere with -1
missing.fillna(-1, inplace=True).info()

# dropping the values
dropped_missing = missing.dropna()

dropped_missing

# to remove the duplicate data
dropped_dupplicate = missing.drop_duplicates(inplace=True)

"""**Exercise 2**"""

tiktok.info()

tiktok[
       tiktok.Title.isnull()
]

tiktok.dropna(inplace=True)
tiktok.info()

"""**Basics Statistical Analysis**
- access column by column
- ``dataframe["columnname"].statisticalfunction()``
- ``.min()``
- ``.max()``
- ``.avg()``
- ``.std()``
- ``dataframe.keys()``
- ``dataframe.loc[:, ["column1", "column2"]].max()``: returning multiple column name
- ``.var()``
- ``.describe()``: return all statistically
- **passing in custom function**
- ```import numpy as np
  def f(x):
    return np.sqrt(x)
  ```
- ``dataframe.loc[:, ["column1", "column2"]].apply(f)``
- used when you o things like root-mean-square, there is no custom function in it

**Statistical Correlation**
- ``dataframe.corr(method="spearman")``
- "help" search for documentation

**Exercise 3**
"""

tiktok.keys()

tiktok["Views avg."].sum()

height.sort_values("Rank", inplace=True, ignore_index=True, ascending=True) # default is asdenging
# do not want to random shuffling

height.loc[50:101, ["Male Height in Cm", "Female Height in Cm"]].avg()

"""**Adding New Column**
- ``dataframe["New Column Name"] = operation like dataframe["columnnameold"] * 12``

**Cascade and shift**
- ``shift(n)``

**Exercise 4**
"""

# must ignore index for shifting
tiktok.sort_values("Subscribers count", inplace=True, ignore_index=True, ascending = True)
tiktok.head(10)

tiktok["delta_sub"] = tiktok['Subscribers count'].shift(1) - tiktok['Subscribers count']

tiktok.head()

"""**Two Dataframe to join together, concatenating but not *merging***
1. Column Join 
2. Row Join

**Exercise 5**
"""

th1 = pd.read_csv("temp_hum_1.csv", parse_dates=["created_at"])
th1.info()
# format conversion for processing
th1.head()
th1.created_at = pd.to_datetime(th1.created_at, utc=True)
th1.drop(columns="Unnamed: 0", axis=1, inplace=True)
th1.rename(
    columns = {
        "field1":"T", 
        "field2":"RH"
    }, inplace = True
)
th1.dropna(inplace=True)
th1.info()

th2 = pd.read_csv("temp_hum_2.csv")
th2.created_at = pd.to_datetime(th2.created_at, utc=True)
th2.drop(columns="Unnamed: 0", axis=1, inplace=True)
th2.rename(
    columns = {
        "field1":"T", 
        "field2":"RH"
    }, inplace = True
)
th2.dropna(inplace=True)

# merge is not the same as concatenation
# default will concatenate the same column name
th = pd.concat([th1, th2], axis=0, ignore_index=False)

th.loc[:, "T":"RH"].describe()

th.T

mn = th.T.mean()
print(mn)

th["delta_average_T"] = th["T"] - th["T"].mean()
th["delta_average_RH"] = th["RH"].mean() - th["RH"]
# to see if it above and below average

th.corr()

"""**Merging Data Frame (Excel)**
1. Pivot Table
2. VLookup (join in SQL)
"""

student = pd.read_csv("student_details.csv")

orders = pd.read_csv("book_orders.csv")

student.join(orders.set_index('ID'), on="ID")

student.merge(orders, on=["ID"], how="inner")
# intersection of A and B
# how include inner, right, left

"""**Queries**"""

height[
       height["Country Name"] == "Malaysia"
       & (height["Male Height in Cm"] <=  120)
]
# return everything satisfy this condition

height.keys()

# uses SQL syntax, does not need to write the syntax
missing.query("Column1 >50 and Column2 <50 and Column3>90")

student.query('Name.str.contains("n")')

student.query('Name.str.contains("^L[A-z]{3}$")')
# regular expression being tested in different software
# to look for a certain patterns

"""**Exercise 6**"""

dictionary.query("word.str.contains('^k') and word.str.contains() and")

"""**Plotting**
- last step in the data
- ``!pip install matplotlib``
"""

import matplotlib.pyplot as plt

th.plot(x="created_at", y=["T", "RH"], kind="scatter")

tiktok.keys()

tiktok.head(10).plot(x="Account", y=["Likes avg", "Comments avg."])

"""**Pivot Table in pandas**
- ``dataframe.groupby(["columnname1", "columnname2"], as_index=False).mean()``
- can export to excel

- Reading dataset with large dataset than your computer memory
- ``import polars as pl``
- alternative to pandas but similar, able to handle large data set
- ``pl.read_csv("filename.csv")``
- integrate well with mathematics and machine learning pipeline (pandas)
- transfer pandas into polars
- integrate into "PySpark"

**Excel** does not have good external library, like double integral

**Matlab** can do the same thing, very hard to commercialize matlab, it has similar functions and capabilities

**Pandas** is more efficient and faster and help you with the analysis

**Pandas** need to run a function to fetch and run the code real time

**Regression** need to be doing in **Scipy** (scientific skills) like numpy and regression, pandas is just read, and pass the value into other libraries for plotting
"""

